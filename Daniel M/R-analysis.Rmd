---
title: "R-analysis"
output: html_document
date: "2025-10-28"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(fs)
library(readr)
library(ggplot2)
library(knitr)
library(dplyr)
library(caret)
library(pROC)
```

## The setup


```{r}
data <- read_csv(path_wd("..", "data", "processed", "odi_bbb_recent.csv"))
```

We are trying to predict the outcome of the game based off the first 10 Overs. Hence, we can only train our model on data we would have by the end of the first 10 Overs, such as, what teams are playing, what is the run rate so far, how many wickets have been taken so far etc. This is to prevent data leakage, ie, it would be extremely easy to work out who would have won the game at the end when we have a final score. This kind of problem is similar to how sports betting companies set odds. Therefore, we are not only interested in the outcome of the game but rather how confident we are in that outcome (hence accuracy is not a suitable measure of performance for our models).

To make this a binary classification problem, we consider whether team1 wins or not. This is obviously equivalent to predicting who won the game. First I filter the data so we only consider the first 10 overs of the first innings (~10% of the game). 

```{r}
first10_overs <- data %>%
  mutate(over = floor(ball)) %>%
  filter(innings == 1, over >= 0, over < 10)

#Now summarise the game info we have so far:

#Per Over summary (for feature engineering later)

over_summary <- first10_overs %>%
  mutate(runs = (coalesce(runs_off_bat, 0) + coalesce(extras, 0))) %>%
  group_by(match_id, over) %>%
  summarise(
    over_runs = sum(runs, na.rm = TRUE),
    over_boundaries = sum((runs_off_bat %in% c(4,6)), na.rm = TRUE),
    over_wickets = sum(!is.na(player_dismissed)),
    over_dotballs = sum((coalesce(runs_off_bat,0) == 0), na.rm = TRUE),
    balls_in_over = n(),
    .groups = "drop"
  )


#Summary of the first 10 overs

first10_summary <- first10_overs %>%
  mutate(runs = (coalesce(runs_off_bat, 0) + coalesce(extras, 0))) %>%
  group_by(match_id) %>%
  summarise(
    batting_team = first(batting_team),
    bowling_team = first(bowling_team),
    balls_played = n(),                         
    overs_played = 10,
    runs_total = sum(runs, na.rm = TRUE),
    boundaries = sum((runs_off_bat %in% c(4,6)), na.rm = TRUE),
    wickets = sum(!is.na(player_dismissed)),
    dot_balls = sum((coalesce(runs_off_bat,0) == 0), na.rm = TRUE),
    extras_total = sum(coalesce(extras,0), na.rm = TRUE),
    .groups = "drop"
  ) %>%
  mutate(
    run_rate_per_over = runs_total / pmax(overs_played, 1/6),    # avoid division by zero
    boundary_rate = boundaries / pmax(balls_played, 1),
    dot_ball_rate = dot_balls / pmax(balls_played, 1),
    wickets_rate = wickets / pmax(overs_played, 1/6),
    avg_runs_per_wicket = runs_total / pmax(wickets, 1)
  )
```


Now we do some feature engineering to try and get some numerical measure for 'momentum' in the game

```{r}
over_rollups <- over_summary %>%
  group_by(match_id) %>%
  summarise(
    first3_runs = sum(over_runs[over %in% 0:2], na.rm = TRUE),
    last3_runs  = sum(over_runs[over %in% 7:9], na.rm = TRUE),
    first3_boundaries = sum(over_boundaries[over %in% 0:2], na.rm = TRUE),
    last3_boundaries  = sum(over_boundaries[over %in% 7:9], na.rm = TRUE),
    first3_balls = sum(balls_in_over[over %in% 0:2], na.rm = TRUE),
    last3_balls = sum(balls_in_over[over %in% 7:9], na.rm = TRUE),
    .groups = "drop"
  ) %>%
  mutate(
    first3_rr = first3_runs / pmax(first3_balls / 6, 1/6),
    last3_rr  = last3_runs  / pmax(last3_balls / 6, 1/6),
    acceleration = last3_rr - first3_rr,
    boundary_momentum = (last3_boundaries / pmax(last3_balls,1)) - (first3_boundaries / pmax(first3_balls,1))
  )
```

```{r}
#Now we take the info we know prior to the match beginning:

match_meta <- data %>%
  distinct(match_id, team1, team2, winner, toss_winner, toss_decision) %>%
  group_by(match_id) %>%
  summarise(
    team1 = first(team1),
    team2 = first(team2),
    winner = first(winner),
    toss_winner = first(toss_winner),
    toss_decision = first(toss_decision),
    .groups = "drop"
  )
```

Now combine it to make our model data frame:

```{r}
model_df <- first10_summary %>%
  left_join(over_rollups, by = "match_id") %>%
  left_join(match_meta, by = "match_id")


glimpse(model_df)
```

Clearly we have a lot of columns/ features here and not all of them will have predictive value. Therefore we will have to do some form of feature selection or regularization process. Furthermore, consider the pairs plot of some of this data, we can see they are highly correlated. For example, runs total and run rate contain the same information just scaled and so the model using both is redundant and could possibly lead to over inflated effects of increased runs (ie both run rate and runs increases causing a larger effect than is true).

```{r}
pairs(model_df[, c("runs_total", "run_rate_per_over", "wickets", 
                   "wickets_rate", "boundaries", "boundary_rate")])

```

We also have a number of categorical features (including out target feature, who wins), which we want to convert into numerical data points by one-hot encoding.

```{r}
model_df1 <- model_df %>%
  filter(!is.na(winner)) %>%
  mutate(
    team1_win = ifelse(winner == team1, 1L, 0L),
    team1_win_toss = as.integer(team1 == toss_winner),
    batting_team_is_team1 = as.integer(batting_team == team1),
    toss_decision = as.integer(toss_decision == "field"),
    batting_team = factor(batting_team),
    bowling_team = factor(bowling_team),
    team1 = factor(team1),
    team2 = factor(team2)
    
  )




model_df1 <- model_df1 %>%
  select(match_id, team1, team2, team1_win,
         batting_team_is_team1,
         runs_total, run_rate_per_over, boundaries, boundary_rate,
         wickets, wickets_rate, dot_balls, dot_ball_rate, extras_total,
         first3_rr, last3_rr, acceleration, boundary_momentum,
         team1_win_toss, toss_decision)

```

We now have 27 columns due to all the different teams and seasons. This is a very large number considering 

```{r}
dim(model_df1)
```
```{r}
cat("Rows (matches) in modeling data:", nrow(model_df1), "\n")
model_df1 %>% count(team1_win) %>% print()
```
We can see that team1 inherently wins slightly more often. From the EDA done previously this is because every home team is listed as team1, (although not every team1 is at home, they could both be away) and hence there is an implicit, slight home bias in this data.


# Linear Classification Methods

This section will focus on applying linear classification methods such as logistic regression and linear SVM. I have chosen these methods because the data set is small and quite noisy. However, there are clear non-linearities we need to capture by this model. If team1 is batting first - then wickets are a bad things and runs good, the opposite is true in the reverse case. Therefore we can try to capture this using interactions. 


Lets fit an initial logistic model.

```{r}
model_df1$team1_win <- as.numeric(model_df1$team1_win)

# Split into train/test (80/20)
set.seed(123)
trainIndex <- createDataPartition(model_df1$team1_win, p = 0.8, list = FALSE)
train_data <- model_df1[trainIndex, ]
test_data  <- model_df1[-trainIndex, ]

# Fit logistic regression model
# Single logistic regression with interactions
logit_model <- glm(
  team1_win ~  . - match_id,
  data = train_data,
  family = binomial
)


summary(logit_model)

```

Here we have some redundant because some predictors are perfect linear combinations of the other. For example acceleration, 

$$\text{acceleration} = \text{last3_rr}-\text{first3_rr}$$



Hence we will drop all redundant (non-rate) variables.

```{r}
train_data <- train_data %>%
  select(-last3_rr)

test_data <- test_data %>%
  select(-last3_rr)
```


```{r}
# Fit logistic regression model
logit_model <- glm(
  team1_win ~ batting_team_is_team1 * (wickets_rate + dot_ball_rate + run_rate_per_over + acceleration + boundary_momentum + boundary_rate + first3_rr) + . - match_id,
  data = train_data,
  family = binomial
)


summary(logit_model)
```



```{r}
# Predict probabilities on test set
pred_probs <- predict(logit_model, newdata = test_data, type = "response")

# Convert to class predictions (threshold = 0.5)
pred_classes <- ifelse(pred_probs > 0.5, 1, 0)

# Confusion matrix
confusionMatrix(
  as.factor(pred_classes),
  as.factor(test_data$team1_win),
  positive = "1"
)

# ROC curve and AUC
roc_obj <- roc(test_data$team1_win, pred_probs)

# Plot ROC curve
plot(roc_obj, col = "blue", lwd = 2, main = "ROC Curve - Logistic Regression")
abline(a = 0, b = 1, lty = 2, col = "gray")

# AUC value
auc_value <- auc(roc_obj)
print(paste("AUC =", round(auc_value, 3)))
```

This is encouraging, we have a relatively large (close to 1) AUC, and we haven't done any feature selection yet. We will now perform feature selection through L1 regularization (LASSO). This penalty that is added can force the coefficient of some features to 0, effectively performing feature selection. It appears currently that many of our features are irrelevant (large p-value on t-test) and so this should help prevent over fitting by only selecting important variables.

```{r}
library(glmnet)

# Prepare data
x_train <- model.matrix(team1_win ~ batting_team_is_team1 * (wickets_rate + dot_ball_rate + run_rate_per_over + acceleration + boundary_momentum + boundary_rate +first3_rr) + . - match_id, data = train_data)[, -1]  # remove intercept column
y_train <- train_data$team1_win

x_test <- model.matrix(team1_win ~ batting_team_is_team1 * (wickets_rate + dot_ball_rate + run_rate_per_over + acceleration + boundary_momentum + boundary_rate + first3_rr) + . - match_id, data = test_data)[, -1]  # remove intercept column

y_test <- test_data$team1_win

lambda_seq <- 10^seq(-6, 2, length=200)

# Fit LASSO logistic regression (L1 penalty)
set.seed(123)
cv_lasso <- cv.glmnet(
  x_train, y_train,
  alpha = 1,              # LASSO penalty (L1)
  lambda = lambda_seq,
  family = "binomial",
  type.measure = "auc",   # use AUC for CV performance
  nfolds = 10             # 10-fold cross-validation
)

# Plot cross-validation results
plot(cv_lasso)

# Best lambda values
cv_lasso$lambda.min     # lambda that gives best mean cross-validated AUC
cv_lasso$lambda.1se     # largest lambda within 1 SE of best AUC (more regularization)

# Refit model at best lambda
lasso_model <- glmnet(x_train, y_train, alpha = 1, family = "binomial", lambda = cv_lasso$lambda.min)




```

```{r}
coef(lasso_model)
```


```{r}
# Predict probabilities on test set
# pred_probs <- predict(lasso_model, newx= x_test, type = "response")


```

The columns don't match up because there is no Jersey in the x test set. 

```{r}
# Identify missing columns
missing_cols <- setdiff(colnames(x_train), colnames(x_test))
missing_cols


# Add missing columns to test matrix
for (col in missing_cols) {
  x_test <- cbind(x_test, setNames(data.frame(rep(0, nrow(x_test))), col))
}

# Ensure column order matches training
x_test <- x_test[, colnames(x_train)]


x_test <- as.matrix(x_test)
```
```{r}
# Predict probabilities on test set
pred_probs <- predict(lasso_model, newx= x_test, type = "response")

# Convert to class predictions (threshold = 0.5)
pred_classes <- ifelse(pred_probs > 0.5, 1, 0)

# Confusion matrix
confusionMatrix(
  as.factor(pred_classes),
  as.factor(test_data$team1_win),
  positive = "1"
)

# ROC curve and AUC
roc_obj <- roc(test_data$team1_win, pred_probs)

# Plot ROC curve
plot(roc_obj, col = "blue", lwd = 2, main = "ROC Curve - Logistic Regression")
abline(a = 0, b = 1, lty = 2, col = "gray")

# AUC value
auc_value <- auc(roc_obj)
print(paste("AUC =", round(auc_value, 3)))
```

# Sensitivity test model:

```{r}
# Start with all columns = 0
scenario <- as.data.frame(matrix(0, nrow = 1, ncol = ncol(train_data)))
colnames(scenario) <- colnames(train_data)

# Set categorical indicators
scenario$team1 <- factor("Australia", levels = levels(train_data$team1))
scenario$team2 <- factor("Canada", levels = levels(train_data$team2))


# Set contextual features
scenario$batting_team_is_team1 <- 1
scenario$runs_total <- 40         # runs after 10 overs
scenario$run_rate_per_over <- 4
scenario$boundaries <- 8
scenario$boundary_rate <- 0.8
scenario$wickets <- 7
scenario$wickets_rate <- 0.7      # 5 wickets / 10 overs
scenario$dot_balls <- 30
scenario$dot_ball_rate <- 0.3
scenario$extras_total <- 2
scenario$first3_rr <- 5.5
scenario$acceleration <- 0.1
scenario$boundary_momentum <- -0.2
scenario$team1_win_toss <- 1
scenario$toss_decision <- 0

```

```{r}
x_scenario <- model.matrix(
  team1_win ~ batting_team_is_team1 * (wickets_rate + dot_ball_rate +
    run_rate_per_over + acceleration + boundary_momentum + boundary_rate + first3_rr) + . - match_id,
  data = scenario
)[, -1]

prob_win <- predict(lasso_model, newx = x_scenario, type = "response")
prob_win
```

Instead predict whether batting team wins:


```{r}
model_df2 <- model_df %>%
  filter(!is.na(winner)) %>%
  mutate(
    batting_team_win = ifelse(winner == batting_team, 1L, 0L),
    batting_team_win_toss = as.integer(batting_team == toss_winner),
    batting_team_is_team1 = as.integer(batting_team == team1),
    batting_team = factor(batting_team),
    bowling_team = factor(bowling_team),
    team1 = factor(team1),
    team2 = factor(team2)
    
  )




model_df2 <- model_df2 %>%
  select(match_id, batting_team, bowling_team, batting_team_win,
         runs_total, run_rate_per_over, boundaries, boundary_rate,
         wickets, wickets_rate, dot_balls, dot_ball_rate, extras_total,
         first3_rr, last3_rr, acceleration, boundary_momentum,
         batting_team_win_toss, batting_team_is_team1)
```


```{r}
model_df2$batting_team_win <- as.numeric(model_df2$batting_team_win)

# Split into train/test (80/20)
set.seed(123)
trainIndex <- createDataPartition(model_df2$batting_team_win, p = 0.8, list = FALSE)
train_data2 <- model_df2[trainIndex, ]
test_data2  <- model_df2[-trainIndex, ]

# Fit logistic regression model
# Single logistic regression with interactions
logit_model2 <- glm(
  batting_team_win ~  . - match_id,
  data = train_data2,
  family = binomial
)


summary(logit_model2)
```
```{r}
train_data2 <- train_data2 %>%
  select(-last3_rr, -run_rate_per_over, -wickets_rate)


test_data2 <- test_data2 %>%
  select(-last3_rr, -run_rate_per_over, -wickets_rate)
```

```{r}
logit_model2 <- glm(
  batting_team_win ~  . - match_id,
  data = train_data2,
  family = binomial
)


summary(logit_model2)
```
```{r}
# Predict probabilities on test set
pred_probs <- predict(logit_model2, newdata = test_data2, type = "response")

# Convert to class predictions (threshold = 0.5)
pred_classes <- ifelse(pred_probs > 0.5, 1, 0)

# Confusion matrix
confusionMatrix(
  as.factor(pred_classes),
  as.factor(test_data2$batting_team_win),
  positive = "1"
)

# ROC curve and AUC
roc_obj <- roc(test_data2$batting_team_win, pred_probs)

# Plot ROC curve
plot(roc_obj, col = "blue", lwd = 2, main = "ROC Curve - Logistic Regression")
abline(a = 0, b = 1, lty = 2, col = "gray")

# AUC value
auc_value <- auc(roc_obj)
print(paste("AUC =", round(auc_value, 3)))
```

```{r}
# Start with all columns = 0
scenario <- as.data.frame(matrix(0, nrow = 1, ncol = ncol(train_data2)))
colnames(scenario) <- colnames(train_data2)

# Set categorical indicators
scenario$batting_team <- factor("Australia", levels = levels(train_data2$batting_team))
scenario$bowling_team <- factor("Canada", levels = levels(train_data2$bowling_team))

# Set contextual features
scenario$runs_total <- 40         # runs after 10 overs
scenario$boundaries <- 8
scenario$boundary_rate <- 0.8
scenario$wickets <- 7     
scenario$dot_balls <- 30
scenario$dot_ball_rate <- 0.3
scenario$extras_total <- 2
scenario$first3_rr <- 5.5
scenario$acceleration <- 0.1
scenario$boundary_momentum <- -0.2
scenario$batting_team_win_toss <- 1


x_scenario2 <- model.matrix(batting_team_win ~ . - match_id, data = scenario)[, -1]
```

```{r}
prob_win <- predict(logit_model2, newdata = scenario, type = "response")
prob_win

```
```{r}
library(glmnet)

# Prepare data
x_train2 <- model.matrix(batting_team_win ~ . - match_id, data = train_data2)[, -1]  # remove intercept column
y_train2 <- train_data2$batting_team_win

x_test2 <- model.matrix(batting_team_win ~ . - match_id, data = test_data2)[, -1]  # remove intercept column

y_test2 <- test_data2$batting_team_win

lambda_seq <- 10^seq(-6, 2, length=200)

# Fit LASSO logistic regression (L2 penalty)
set.seed(123)
cv_lasso2 <- cv.glmnet(
  x_train, y_train,
  alpha = 0,              # Ridge-Regression penalty (L2)
  lambda = lambda_seq,
  family = "binomial",
  type.measure = "auc",   # use AUC for CV performance
  nfolds = 10             # 10-fold cross-validation
)

# Plot cross-validation results
plot(cv_lasso2)

# Best lambda values
cv_lasso2$lambda.min     # lambda that gives best mean cross-validated AUC

# Refit model at best lambda
lasso_model2 <- glmnet(x_train2, y_train2, alpha = 1, family = "binomial", lambda = cv_lasso2$lambda.min)




```



```{r}
coef(lasso_model2)
```



```{r}
# Predict probabilities on test set
pred_probs <- predict(lasso_model2, newx= x_test2, type = "response")

# Convert to class predictions (threshold = 0.5)
pred_classes <- ifelse(pred_probs > 0.5, 1, 0)



# ROC curve and AUC
roc_obj <- roc(test_data2$batting_team_win, pred_probs)

# Plot ROC curve
plot(roc_obj, col = "blue", lwd = 2, main = "ROC Curve - Logistic Regression")
abline(a = 0, b = 1, lty = 2, col = "gray")

# AUC value
auc_value <- auc(roc_obj)
print(paste("AUC =", round(auc_value, 3)))
```
```{r}
prob_win <- predict(lasso_model2, newx = x_scenario2, type = "response")
prob_win
```





```{r}
# # Prepare data
# x_train2 <- model.matrix(batting_team_win ~ . - match_id, data = train_data2)[, -1]  # remove intercept column
# y_train2 <- train_data2$batting_team_win
# 
# x_test2 <- model.matrix(batting_team_win ~ . - match_id, data = test_data2)[, -1]  # remove intercept column
# 
# y_test2 <- test_data2$batting_team_win
# 
# # Convert to XGBoost DMatrix format
# dtrain <- xgb.DMatrix(data = x_train2, label = y_train2)
# dtest  <- xgb.DMatrix(data = x_test2, label = y_test2)
# 
# #---------------------------------
# # 2. Cross-validated Training
# #---------------------------------
# params <- list(
#   objective = "binary:logistic",  # logistic regression for classification
#   eval_metric = "auc",            # use AUC for performance
#   booster = "gbtree",
#   eta = 0.05,                     # learning rate
#   max_depth = 6,
#   subsample = 0.8,
#   colsample_bytree = 0.8
# )
# 
# set.seed(123)
# xgb_cv <- xgb.cv(
#   params = params,
#   data = dtrain,
#   nrounds = 500,
#   nfold = 5,
#   early_stopping_rounds = 20,
#   verbose = 0
# )
# 
# best_nrounds <- xgb_cv$best_iteration
# 
# #---------------------------------
# # 3. Fit Final Model
# #---------------------------------
# xgb_model <- xgb.train(
#   params = params,
#   data = dtrain,
#   nrounds = best_nrounds,
#   watchlist = list(train = dtrain, test = dtest),
#   verbose = 0
# )
# 
# #---------------------------------
# # 4. Predictions & Performance
# #---------------------------------
# pred_probs <- predict(xgb_model, dtest)
# pred_labels <- ifelse(pred_probs > 0.5, 1, 0)
# 
# auc_score <- roc(y_test2, pred_probs)$auc
# cat("Test AUC:", auc_score, "\n")
# 
# #---------------------------------
# # 5. Feature Importance
# #---------------------------------
# importance <- xgb.importance(model = xgb_model)
# xgb.plot.importance(importance)
```
```{r}

# #-------------------------------------------------------------
# # Random Search Parameter Grid
# #-------------------------------------------------------------
# set.seed(42)
# 
# n_search <- 50  # number of random combinations
# param_grid <- data.frame(
#   eta = runif(n_search, 0.01, 0.5),
#   max_depth = sample(3:10, n_search, replace = TRUE),
#   subsample = runif(n_search, 0.6, 1),
#   colsample_bytree = runif(n_search, 0.6, 1)
# )
# 
# results <- data.frame()
# 
# #-------------------------------------------------------------
# # Random Search Cross-Validation
# #-------------------------------------------------------------
# for (i in 1:n_search) {
#   params <- list(
#     objective = "binary:logistic",
#     eval_metric = "auc",
#     eta = param_grid$eta[i],
#     max_depth = param_grid$max_depth[i],
#     subsample = param_grid$subsample[i],
#     colsample_bytree = param_grid$colsample_bytree[i]
#   )
#   
#   cv <- xgb.cv(
#     params = params,
#     data = dtrain,
#     nrounds = 500,
#     nfold = 5,
#     early_stopping_rounds = 30,
#     verbose = 0
#   )
#   
#   best_iter <- cv$best_iteration
#   best_auc <- cv$evaluation_log$test_auc_mean[best_iter]
#   
#   results <- rbind(results, cbind(param_grid[i, ], best_iter, best_auc))
# }
# 
# #-------------------------------------------------------------
# # Choose Best Parameters
# #-------------------------------------------------------------
# best_params <- results[which.max(results$best_auc), ]
# print(best_params)
# 
# #-------------------------------------------------------------
# # Train Final Model with Best Parameters
# #-------------------------------------------------------------
# final_params <- list(
#   objective = "binary:logistic",
#   eval_metric = "auc",
#   eta = best_params$eta,
#   max_depth = best_params$max_depth,
#   subsample = best_params$subsample,
#   colsample_bytree = best_params$colsample_bytree
# )
# 
# xgb_final <- xgb.train(
#   params = final_params,
#   data = dtrain,
#   nrounds = best_params$best_iter,
#   watchlist = list(train = dtrain, test = dtest),
#   print_every_n = 20,
#   early_stopping_rounds = 30
# )
# 
# #-------------------------------------------------------------
# # Evaluate Performance
# #-------------------------------------------------------------
# pred_probs <- predict(xgb_final, dtest)
# auc_test <- roc(y_test2, pred_probs)$auc
# cat("\nTest AUC:", round(auc_test, 4), "\n")
```

Overfitting so include regularization etc.

```{r}
library(glmnet)

# Prepare data
x_train2 <- model.matrix(batting_team_win ~ . - match_id, data = train_data2)[, -1]  # remove intercept
y_train2 <- train_data2$batting_team_win

x_test2 <- model.matrix(batting_team_win ~ . - match_id, data = test_data2)[, -1]  # remove intercept
y_test2 <- test_data2$batting_team_win

# Grid of alpha values to try
alpha_grid <- seq(0, 1, by = 0.1)  # from Ridge (0) to LASSO (1)
lambda_seq <- 10^seq(-6, 2, length = 200)

# Store CV results
cv_results <- list()
cv_auc <- numeric(length(alpha_grid))

set.seed(123)
for (i in seq_along(alpha_grid)) {
  cv <- cv.glmnet(
    x_train2, y_train2,
    alpha = alpha_grid[i],
    lambda = lambda_seq,
    family = "binomial",
    type.measure = "auc",
    nfolds = 10
  )
  cv_results[[i]] <- cv
  cv_auc[i] <- max(cv$cvm)  # best AUC for this alpha
}

# Find alpha with best CV AUC
best_alpha <- alpha_grid[which.max(cv_auc)]
best_lambda <- cv_results[[which.max(cv_auc)]]$lambda.min

cat("Best alpha:", best_alpha, "\n")
cat("Best lambda:", best_lambda, "\n")

# Refit final elastic net model on full training set
final_model <- glmnet(
  x_train2, y_train2,
  alpha = best_alpha,
  lambda = best_lambda,
  family = "binomial"
)

# Predict on test set
test_pred_prob <- predict(final_model, newx = x_test2, type = "response")
test_pred_class <- ifelse(test_pred_prob > 0.5, 1, 0)

# Evaluate test AUC
library(pROC)
roc_obj <- roc(y_test2, as.numeric(test_pred_prob))
auc(roc_obj)

```

