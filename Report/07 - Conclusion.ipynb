{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b06a1c85",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "We have explore Logistic Regression, Support Vector Machines (SVM), Random Forests and General Linear Mixed Models (GLMM) to predict match outcomes for Men's ODI cricket matches. We all did our own feature engineering and selection to suit the model we were using, so this is a limiting factor when comparing model performance. However, across all models, the data from the first innings and first 10 overs of the second innings were used for prediction, so we have consistency there. We will now compare the models based on their ROC curves and AUC scores, which were saved as CSV files during model fitting. We use the ROC curves for reasons discussed in the introduction in this report. All models were evaluated on an 80/20 train-test split of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3aaa35e",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Read in each model's ROC data \u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Read in each model's ROC data \n",
    "svm = pd.read_csv(\"ROC data/roc_svm.csv\")\n",
    "logistic = pd.read_csv(\"ROC data/roc_logistic.csv\")\n",
    "glmm = pd.read_csv(\"ROC data/roc_glmm.csv\")\n",
    "rf = pd.read_csv(\"ROC data/roc_rf.csv\")\n",
    "\n",
    "\n",
    "# Plot all ROC curves\n",
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "plt.plot(svm[\"FPR\"], svm[\"TPR\"], label=\"SVM (AUC = 0.787)\")\n",
    "plt.plot(logistic[\"fpr\"], logistic[\"tpr\"], label=\"Regularized Logistic Regression\")\n",
    "plt.plot(glmm[\"FPR\"], glmm[\"TPR\"], label=\"GLMM\")\n",
    "plt.plot(rf[\"FPR\"], rf[\"TPR\"], label=\"Random Forest (AUC = 0.834\")\n",
    "\n",
    "\n",
    "plt.plot([0, 1], [0, 1], \"k--\", lw=1)\n",
    "\n",
    "# Labels, title, legend\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Comparison of ROC Curves Across Models\")\n",
    "plt.legend(title=\"Models\")\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742604db",
   "metadata": {},
   "source": [
    "## Discussion of Results\n",
    "\n",
    "We can see that the worst performing model is the SVM with linear kernel, with an AUC score of 0.787. This makes sense, as it does not capture non-linear relationships in the data. The Random Forest model is particularly good at this, and is also much more robust to outliers, which may explain why it performed better with an AUC score of 0.834. The Logistic Mixed effect model performed better than the linear SVM as well, which also we would expect given its ability to account for random effects in the data, as well as hierarchical structures of the data (e.g. same players, teams, venues etc) achieving an AUC of 0.761. In comparison with the logistic regression model without regularisation, the logistic mixed effects model performed just the same achieving the same AUC, which is suprising considering that the addition of random effects did not improve this value. Our best performing model was the Regularized Logistic Regression, with an AUC score of 0.909. One of the benefits of this model is that it can handle a sparse feature space well, as it penalizes the coefficients of less important features, effectively performing feature selection. I think this is particularly useful in our case, as the cricket data set had a large number of features, especially after one-hot encoding the qualitative variables such as venue and team names. \n",
    "\n",
    "It's important to note that all of our models were trained on only the first innings and the first 10 overs of the second innings, which is a small proportion of the total match, and therefore we would expect to see even better performance as the match progresses."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
