---
title: "Model Implementation"
author: "Youssef Othman"
date: "2025-11-02"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
### Install and load required packages:
```{r}
pkgs <- c("lme4", "ggplot2", "pROC", "caret", "dplyr")

for (p in pkgs) {
  if (!require(p, character.only = TRUE)) install.packages(p)
  library(p, character.only = TRUE)
}
```





We load in the engineered data from python :
- To do this ensure you have set your working directory to be in the same place
as the csv. (If yiu have ran python script first should be the case)

```{r}
# Load both datasets
data1 <- read.csv("cricket_model_data_10.csv")
data2 <- read.csv("cricket_model_data_60.csv")


# Convert to factors where appropriate
data1$season <- as.factor(data1$season)
data1$team1 <- as.factor(data1$team1)
data1$team2 <- as.factor(data1$team2)
data1$venue <- as.factor(data1$venue)
data1$winner_binary <- as.factor(data1$winner_binary)

# Convert to factors where appropriate
data2$season <- as.factor(data2$season)
data2$team1 <- as.factor(data2$team1)
data2$team2 <- as.factor(data2$team2)
data2$venue <- as.factor(data2$venue)
data2$winner_binary <- as.factor(data2$winner_binary)
```

We split both datasets using the normal test-train split:
```{r}
#Test-Train Split:

set.seed(123)

train_indices1 <- createDataPartition(data1$winner_binary, p = 0.8, list = FALSE)
train_data1 <- data1[train_indices1, ]
test_data1 <- data1[-train_indices1, ]

train_indices2 <- createDataPartition(data2$winner_binary, p = 0.8, list = FALSE)
train_data2 <- data2[train_indices2, ]
test_data2 <- data2[-train_indices2, ]

```
Then we scale the data to stop the following error from popping up when we try
to fit the model:

`Warning in checkConv(attr(opt, "derivs"), optpar, ctrl = controlcheckConv,  :
  Model is nearly unidentifiable: very large eigenvalue.
-Rescale variables'


This then also makes the models run much quicker.
```{r}
# Identify numeric predictors (don’t include the binary response or factors)
num_vars1 <- c("team1_wickets","team1_dot_ball_percentage","team1_run_rate")
num_vars2 <- c("wickets_difference", "runs_difference", "run_rate_difference",
              "team1_dot_ball_percentage" )

# Compute scaling parameters from training data
train_means <- sapply(train_data2[num_vars2], mean, na.rm = TRUE)
train_sds   <- sapply(train_data2[num_vars2], sd,   na.rm = TRUE)

# Apply scaling to both train and test using training parameters
train_scaled <- train_data2
test_scaled  <- test_data2

train_scaled[num_vars2] <- scale(train_data2[num_vars2], center = train_means, scale = train_sds)
test_scaled[num_vars2]  <- scale(test_data2[num_vars2],  center = train_means, scale = train_sds)


```




```{r} 
#Test train split without temporal leakage:

# Simple and effective time-based split
set.seed(123)
seasons <- sort(unique(data1$season))
train_seasons <- seasons[1:round(length(seasons) * 0.8)]
test_seasons <- seasons[!seasons %in% train_seasons]

train_temp1 <- data1[data1$season %in% train_seasons, ]
test_temp1<- data1[data1$season %in% test_seasons, ]

train_temp2 <- data2[data2$season %in% train_seasons, ]
test_temp2<- data2[data2$season %in% test_seasons, ]

# Verify the split preserves team structure
cat("Unique teams in training:", length(unique(c(train_temp1$team1, train_temp1$team2))), "\n")
cat("Unique teams in test:", length(unique(c(test_temp1$team1, test_temp1$team2))), "\n")
```

# Specifying the Random Effects:

Now that we have chosen our fixed effects all is left is to choose the random 
effects for our model. We consider 5 different options for our model : 3 are 
just simple random intercepts, one has a random slope parameter and one has only
the fixed effects. 

```{r}
fixed_effects <- "runs_difference + wickets_difference + team1_dot_ball_percentage + run_rate_difference + toss_won_by_team1"

# ============================================================
# Random effects candidates
# ============================================================

# Option 1: Random intercepts for team1 and team2
formula1 <- as.formula(paste(c("winner_binary ~", fixed_effects, "+ (1 | team1)",
                             "+ (1 | team2)"),
                       collapse = " "))

# Option 2: Add season as random intercept
formula2 <- as.formula(paste(c("winner_binary ~", fixed_effects, "+ (1 | season)",
                               "+ (1 | team1)", "+ (1 | team2)"),
                             collapse = " "))

# Option 3: Add venue as another grouping factor
formula3 <- as.formula(paste(c("winner_binary ~", fixed_effects, "+ (1 | season)",
                             "+ (1 | venue)", "+ (1 | team1)", "+ (1 | team2)"),
                             collapse = " "))

# Option 3.1: Add random slope for run_rate_difference by team 
formula3.1 <- as.formula(paste(c("winner_binary ~", fixed_effects, 
                               "+ (1 + run_rate_difference | team1)", 
                               "+(1 + run_rate_difference | team2)", 
                               "+ (1 | season)", "+ (1 | venue)"),
                         collapse = " "))

# Option 4: No random effects (pure GLM baseline)
formula4 <- as.formula(paste("winner_binary ~", fixed_effects))
```



## Fitting the models:

We fit the various models and check if any models don't converge or pop up with 
'isSingular'.
```{r}
# ============================================================
# Fit all models
# ============================================================

model1 <- glmer(formula1, data = train_scaled, family = binomial(link = "logit"),
                control = glmerControl(optimizer = "bobyqa", optCtrl = list(maxfun = 100000)))

model2 <- glmer(formula2, data = train_scaled, family = binomial(link = "logit"),
                control = glmerControl(optimizer = "bobyqa", optCtrl = list(maxfun = 100000)))

model3 <- glmer(formula3, data = train_scaled, family = binomial(link = "logit"),
                control = glmerControl(optimizer = "bobyqa", optCtrl = list(maxfun = 100000)))

# Random slope version (only if computationally feasible)
model3.1 <- tryCatch({
  glmer(formula3.1, data = train_scaled, family = binomial(link = "logit"),
        control = glmerControl(optimizer = "bobyqa", optCtrl = list(maxfun = 100000)))
}, error = function(e) { message("Model 3.1 failed to converge"); return(NULL) })

# GLM version
model4 <- glm(formula4, data = train_scaled, family = binomial(link = "logit"))
```
Here we see that the GLMM with random slopes produced an 'isSingular' error 
which according to help('isSingular') this error occurs when the variances of 
one or more linear combinations of effects are close to zero. Which we therefore
dismiss.


### Model Comparison with AIC/BIC:
Here we look at the AIC/BIC values of the models and take this into account when 
we decide between our models.
```{r}
print(" MODEL COMPARISON")

models <- list(Model1 = model1, Model2 = model2, Model3 = model3, Model3.1 = model3.1, Model4 = model4)

aic_values <- sapply(models, function(m) if (!is.null(m)) AIC(m) else NA)
bic_values <- sapply(models, function(m) if (!is.null(m)) BIC(m) else NA)

print("AIC values:")
print(aic_values)
print(paste("Best model by AIC:", names(which.min(aic_values))))

print("BIC values:")
print(bic_values)
print(paste("Best model by BIC:", names(which.min(bic_values))))
```
From this output, it suggests that the first model which only has 'team1' and 
'team2' as the random effect performs best in the BIC or the second model which
incorporates the 'season' variable as a random effect that peforms best in the 
AIC. They are both close in terms of these metrics with not much seperating 
Model1, Model2 and Model3. 

However, one thing we can say is that Model3.1 and Model4 do perform noticeably 
worse. This further justifies that incorporating these random effects are 
justified in terms of model selection. 


## Model selection with Variance and ICC:

### Intraclass Correlation Coefficient (ICC) in Mixed Models

The Intraclass Correlation Coefficient (ICC) measures the proportion of the total variance in the response that is attributable to differences between groups (i.e., the random effects).  
In a mixed model, it quantifies how strongly units within the same group resemble each other.

---

#### General definition

For a standard linear mixed model of the form

$$
y_{ij} = \beta_0 + u_j + \varepsilon_{ij},
$$

where $u_j \sim \mathcal{N}(0, \sigma_u^2)$ represents the between-group variance and  
$\varepsilon_{ij} \sim \mathcal{N}(0, \sigma_\varepsilon^2)$ represents the within-group variance,  
the ICC is defined as

$$
\text{ICC} = \frac{\sigma_u^2}{\sigma_u^2 + \sigma_\varepsilon^2}.
$$

A higher ICC indicates stronger within-group similarity, implying that group-level effects account for a larger share of total variability.

---

#### ICC in the logistic mixed model

In a logistic mixed-effects model, the outcome is binary, and the level-1 residual variance $\sigma_\varepsilon^2$ is not directly estimated because it depends on the logistic distribution.  
Following standard practice (Snijders & Bosker, 2012)[1], this variance is approximated as

$$
\sigma_\varepsilon^2 = \frac{\pi^2}{3} \approx 3.29.
$$

For a random-intercept logistic model

$$
\text{logit}(P(y_{ij}=1)) = \beta_0 + u_j,
$$

where $u_j \sim \mathcal{N}(0, \sigma_u^2)$, the ICC becomes

$$
\text{ICC} = \frac{\sigma_u^2}{\sigma_u^2 + \frac{\pi^2}{3}}.
$$

This represents the proportion of total variance in the latent continuous propensity (log-odds scale) explained by group-level clustering.

---

#### Using the ICC for random effects selection

The ICC can be used as a diagnostic to assess whether including random effects is justified:

- If the ICC is close to zero, between-group variability is negligible, and a fixed-effects model (no random intercepts) may suffice.

- Comparing ICC values for different grouping variables (e.g., team, season, venue) helps identify which random effects meaningfully contribute to the model.



---

```{r}
# ------------------------------------------------------------
# Function to extract variance components & compute ICC
# ------------------------------------------------------------
get_icc_components <- function(model) {
  # Extract random effect variances
  var_comps <- as.data.frame(VarCorr(model))
  var_comps_summary <- var_comps[, c("grp", "vcov")]
  names(var_comps_summary) <- c("Group", "Variance")
  
  # Total random-effect variance
  total_var <- sum(var_comps_summary$Variance)
  
  # Logistic residual variance (latent scale)
  resid_var <- (pi^2) / 3
  
  # ICC per grouping factor
  var_comps_summary$ICC_component <- var_comps_summary$Variance / (total_var + resid_var)
  
  # Overall ICC (total random effect contribution)
  overall_icc <- total_var / (total_var + resid_var)
  
  list(
    random_effect_variances = var_comps_summary,
    total_random_variance = total_var,
    residual_variance = resid_var,
    overall_ICC = overall_icc
  )
}


# ------------------------------------------------------------
# Function to visualize ICC components
# ------------------------------------------------------------
plot_icc_components <- function(icc_results, title = "Proportion of Total Variance (ICC) 
                                by Random Effect") {
  
  # Extract ICC data
  icc_df <- icc_results$random_effect_variances
  
  # Create bar plot
  p <- ggplot(icc_df, aes(x = reorder(Group, ICC_component), y = ICC_component)) +
    geom_col(fill = "steelblue", alpha = 0.8) +
    geom_text(aes(label = round(ICC_component, 3)), hjust = -0.1, size = 3.5) +
    coord_flip() +
    labs(
      title = title,
      subtitle = paste0("Overall ICC: ", round(icc_results$overall_ICC, 3)),
      x = "Random Effect Grouping", 
      y = "ICC Component (Proportion of Variance)"
    ) +
    theme_minimal() +
    theme(
      plot.title = element_text(face = "bold", size = 14),
      plot.subtitle = element_text(size = 11, color = "gray40"),
      axis.title = element_text(size = 11),
      axis.text = element_text(size = 10)
    ) +
    ylim(0, max(icc_df$ICC_component) * 1.15)  # Add space for labels
  
  return(p)
}


# ------------------------------------------------------------
# Function to create variance decomposition pie chart
# ------------------------------------------------------------
plot_variance_decomposition <- function(icc_results) {
  
  # Prepare data
  icc_df <- icc_results$random_effect_variances
  total_var <- sum(icc_df$Variance) + icc_results$residual_variance
  
  # Create dataframe with all variance sources
  var_df <- data.frame(
    Source = c(as.character(icc_df$Group), "Residual"),
    Variance = c(icc_df$Variance, icc_results$residual_variance),
    Proportion = c(icc_df$Variance / total_var, icc_results$residual_variance / total_var)
  )
  
  # Create pie chart
  p <- ggplot(var_df, aes(x = "", y = Proportion, fill = Source)) +
    geom_bar(stat = "identity", width = 1, color = "white") +
    coord_polar("y", start = 0) +
    geom_text(aes(label = paste0(round(Proportion * 100, 1), "%")), 
              position = position_stack(vjust = 0.5), size = 3.5) +
    labs(
      title = "Variance Decomposition",
      subtitle = "Random Effects vs Residual Variance",
      fill = "Variance Source"
    ) +
    theme_void() +
    theme(
      plot.title = element_text(face = "bold", size = 14, hjust = 0.5),
      plot.subtitle = element_text(size = 11, color = "gray40", hjust = 0.5),
      legend.position = "right"
    ) +
    scale_fill_brewer(palette = "Set2")
  
  return(p)
}


# ------------------------------------------------------------
# Function to compare multiple models
# ------------------------------------------------------------
compare_model_icc <- function(model_list, model_names = NULL) {
  
  if (is.null(model_names)) {
    model_names <- paste0("Model", seq_along(model_list))
  }
  
  # Extract ICC for each model
  icc_list <- lapply(model_list, get_icc_components)
  
  # Combine into one dataframe
  combined_df <- do.call(rbind, lapply(seq_along(icc_list), function(i) {
    df <- icc_list[[i]]$random_effect_variances
    df$Model <- model_names[i]
    return(df)
  }))
  
  # Plot comparison
  p <- ggplot(combined_df, aes(x = Group, y = ICC_component, fill = Model)) +
    geom_bar(stat = "identity", position = "dodge", alpha = 0.8) +
    labs(
      title = "ICC Comparison Across Models",
      x = "Random Effect Grouping",
      y = "ICC Component"
    ) +
    theme_minimal() +
    theme(
      plot.title = element_text(face = "bold", size = 14),
      axis.text.x = element_text(angle = 45, hjust = 1)
    ) +
    scale_fill_brewer(palette = "Set1")
  
  return(list(plot = p, data = combined_df))
}
```


```{r}

comparison <- compare_model_icc(
          model_list = list(model1, model2, model3),
          model_names = c("Baseline", "Extended", "Full")
)
print(comparison$data)
print(comparison$plot)
```

We see here that team1 and team2 definitely account for the most variance  
across all of the models therefore these two variables are our prime candidates.

Looking at the ICC Component for season it is around 1.7% of the variance this 
shows that it is contributing very little to the model.

The venue effect is also minimal but is larger than the seasons affect.


## Model Selection by Predictive Measures:

```{r}
# ============================================================================
# Model Evaluation Function for GLMMs
# ============================================================================

evaluate_glmm_model <- function(model, train_data, test_data, 
                                model_name = "GLMM Model",
                                threshold = 0.5) {
  
  # -------------------------------------------------------------------------
  #  Get predictions on training and test data
  # -------------------------------------------------------------------------
  
  # Training predictions
  train_pred_prob <- predict(model, newdata = train_data, type = "response", 
                             allow.new.levels = TRUE)
  train_pred_class <- ifelse(train_pred_prob > threshold, 1, 0)
  train_actual <- train_data$winner_binary
  
  # Test predictions
  test_pred_prob <- predict(model, newdata = test_data, type = "response", 
                            allow.new.levels = TRUE)
  test_pred_class <- ifelse(test_pred_prob > threshold, 1, 0)
  test_actual <- test_data$winner_binary
  
  # -------------------------------------------------------------------------
  #  ROC and AUC
  # -------------------------------------------------------------------------
  
  # Training ROC
  train_roc <- roc(train_actual, train_pred_prob, quiet = TRUE)
  train_auc <- auc(train_roc)
  
  # Test ROC
  test_roc <- roc(test_actual, test_pred_prob, quiet = TRUE)
  test_auc <- auc(test_roc)
  
  # -------------------------------------------------------------------------
  #  Confusion Matrix and derived metrics
  # -------------------------------------------------------------------------
  
  # Training confusion matrix
  train_cm <- table(Predicted = train_pred_class, Actual = train_actual)
  train_accuracy <- sum(diag(train_cm)) / sum(train_cm)
  
  # Test confusion matrix
  test_cm <- table(Predicted = test_pred_class, Actual = test_actual)
  test_accuracy <- sum(diag(test_cm)) / sum(test_cm)
  
  # -------------------------------------------------------------------------
  #  Precision, Recall, F1-Score
  # -------------------------------------------------------------------------
  
  # Training metrics
  train_tp <- train_cm[2, 2]
  train_fp <- train_cm[2, 1]
  train_fn <- train_cm[1, 2]
  train_tn <- train_cm[1, 1]
  
  train_precision <- train_tp / (train_tp + train_fp)
  train_recall <- train_tp / (train_tp + train_fn)
  train_f1 <- 2 * (train_precision * train_recall) / (train_precision + train_recall)
  train_specificity <- train_tn / (train_tn + train_fp)
  
  # Test metrics
  test_tp <- test_cm[2, 2]
  test_fp <- test_cm[2, 1]
  test_fn <- test_cm[1, 2]
  test_tn <- test_cm[1, 1]
  
  test_precision <- test_tp / (test_tp + test_fp)
  test_recall <- test_tp / (test_tp + test_fn)
  test_f1 <- 2 * (test_precision * test_recall) / (test_precision + test_recall)
  test_specificity <- test_tn / (test_tn + test_fp)

  

  
  
  # -------------------------------------------------------------------------
  # Return results
  # -------------------------------------------------------------------------
  
  results <- list(
    model_name = model_name,
    
    # Training metrics
    train = list(
      auc = as.numeric(train_auc),
      accuracy = train_accuracy,
      precision = train_precision,
      recall = train_recall,
      specificity = train_specificity,
      f1_score = train_f1,
      confusion_matrix = train_cm,
      predictions = data.frame(
        actual = train_actual,
        predicted_prob = train_pred_prob,
        predicted_class = train_pred_class
      )
    ),
    
    # Test metrics
    test = list(
      auc = as.numeric(test_auc),
      accuracy = test_accuracy,
      precision = test_precision,
      recall = test_recall,
      specificity = test_specificity,
      f1_score = test_f1,
      confusion_matrix = test_cm,
      predictions = data.frame(
        actual = test_actual,
        predicted_prob = test_pred_prob,
        predicted_class = test_pred_class
      )
    ),
    
    # ROC objects
    roc_train = train_roc,
    roc_test = test_roc
  )

  
  invisible(results)
}
```

Now that we have created a function to create our various measures of accuracy
we output our results and compare them particulary focusing on the AUC metric.


```{r}
res1 <- evaluate_glmm_model(model1, train_scaled, test_scaled, "Baseline")
res2 <- evaluate_glmm_model(model2, train_scaled, test_scaled, "Extended")
res3 <- evaluate_glmm_model(model3, train_scaled, test_scaled, "Hybrid")

# -------------------------------------------------------------------------
#  Create a summary table of all model metrics
# -------------------------------------------------------------------------

summary_table <- bind_rows(
  data.frame(
    Model = res1$model_name,
    Dataset = "Training",
    AUC = res1$train$auc,
    Accuracy = res1$train$accuracy,
    Precision = res1$train$precision,
    Recall = res1$train$recall,
    Specificity = res1$train$specificity,
    F1 = res1$train$f1_score
  ),
  data.frame(
    Model = res1$model_name,
    Dataset = "Test",
    AUC = res1$test$auc,
    Accuracy = res1$test$accuracy,
    Precision = res1$test$precision,
    Recall = res1$test$recall,
    Specificity = res1$test$specificity,
    F1 = res1$test$f1_score
  ),
  data.frame(
    Model = res2$model_name,
    Dataset = "Training",
    AUC = res2$train$auc,
    Accuracy = res2$train$accuracy,
    Precision = res2$train$precision,
    Recall = res2$train$recall,
    Specificity = res2$train$specificity,
    F1 = res2$train$f1_score
  ),
  data.frame(
    Model = res2$model_name,
    Dataset = "Test",
    AUC = res2$test$auc,
    Accuracy = res2$test$accuracy,
    Precision = res2$test$precision,
    Recall = res2$test$recall,
    Specificity = res2$test$specificity,
    F1 = res2$test$f1_score
  ),
  data.frame(
    Model = res3$model_name,
    Dataset = "Training",
    AUC = res3$train$auc,
    Accuracy = res3$train$accuracy,
    Precision = res3$train$precision,
    Recall = res3$train$recall,
    Specificity = res3$train$specificity,
    F1 = res3$train$f1_score
  ),
  data.frame(
    Model = res3$model_name,
    Dataset = "Test",
    AUC = res3$test$auc,
    Accuracy = res3$test$accuracy,
    Precision = res3$test$precision,
    Recall = res3$test$recall,
    Specificity = res3$test$specificity,
    F1 = res3$test$f1_score
  )
)

# Round for display
summary_table <- summary_table %>%
  mutate(across(where(is.numeric), ~ round(.x, 3)))

print(summary_table)

```

From this table we can see that the hybrid model does the best in training and 
it doesn't do as well in the test across all of the metrics considered.

If we purely look at the test AUC metric then the extended model performs the 
best and it seems to be the best fit as it perfomrs second best in the trainning
data and performs the best in the test data. This shows that the model is not
overfitting.


## ROC Curves Visualisation
```{r}
# -------------------------------------------------------------------------
#  Combine ROC curves for visualisation
# -------------------------------------------------------------------------

roc_data_test <- data.frame(
  FPR = c(1 - res1$roc_test$specificities,
          1 - res2$roc_test$specificities,
          1 - res3$roc_test$specificities),
  TPR = c(res1$roc_test$sensitivities,
          res2$roc_test$sensitivities,
          res3$roc_test$sensitivities),
  Model = factor(rep(c(res1$model_name, res2$model_name, res3$model_name),
                     times = c(length(res1$roc_test$specificities),
                               length(res2$roc_test$specificities),
                               length(res3$roc_test$specificities))))
)

# -------------------------------------------------------------------------
# Plot Test ROC curves 
# -------------------------------------------------------------------------
ggplot(roc_data_test, aes(x = FPR, y = TPR, color = Model)) +
  geom_line(linewidth = 1.2) +
  geom_abline(intercept = 0, slope = 1, color = "gray60", linetype = "dashed") +
  labs(
    title = "ROC Curve Comparison of GLMM Models",
    subtitle = "Test Data",
    x = "1 - Specificity (False Positive Rate)",
    y = "Sensitivity (True Positive Rate)"
  ) +
  theme_minimal(base_size = 13) +
  theme(
    plot.title = element_text(face = "bold", size = 14),
    legend.position = "top",   # clean legend above the plot
    legend.title = element_blank(),
    legend.background = element_rect(fill = "white", color = "gray80"),
    panel.grid.minor = element_blank()
  ) +
  scale_color_manual(values = c(
    "Baseline" = "#F8766D",
    "Extended" = "#00BFC4",
    "Hybrid"   = "#7CAE00"
  )) +
  annotate("text", x = 0.6, y = 0.25,
           label = paste0("Baseline AUC = ", round(res1$test$auc, 3)),
           color = "#F8766D", size = 4, hjust = 0) +
  annotate("text", x = 0.6, y = 0.18,
           label = paste0("Extended AUC = ", round(res2$test$auc, 3)),
           color = "#00BFC4", size = 4, hjust = 0) +
  annotate("text", x = 0.6, y = 0.11,
           label = paste0("Hybrid AUC = ", round(res3$test$auc, 3)),
           color = "#7CAE00", size = 4, hjust = 0)
```

Therefore, we conclude that our extended model - which performs well across the 
board in terms of model selection optimal in the sense of AIC,BIC,and 
optimally in the sense of maximizing the test AUC - is our best model for the 
classification task of predicting the outcome of a cricket match based on the 
first innings and the first 10 overs.

#### References

- Snijders, T. A. B., & Bosker, R. J. (2012). *Multilevel Analysis: An Introduction to Basic and Advanced Multilevel Modeling* (2nd ed.). Sage.  
- Goldstein, H., Browne, W., & Rasbash, J. (2002). *Multilevel modelling of medical data*. *Statistics in Medicine*, 21(21), 3291–3315.  
- Nakagawa, S., & Schielzeth, H. (2010). *Repeatability for Gaussian and non-Gaussian data: a practical guide for biologists*. *Biological Reviews*, 85(4), 935–956.  
- Hox, J., Moerbeek, M., & van de Schoot, R. (2017). *Multilevel Analysis: Techniques and Applications* (3rd ed.). Routledge.

