---
title: "Logistic Mixed Effects Model Implementation"
author: "Youssef Othman"
date: "2025-10-30"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
read.csv("cricket_model_data.csv") 
```
```{r}

# Install and load required libraries


required_packages <- c("lme4", "dplyr", "ggplot2", "performance", "sjPlot")

# Install any missing packages
installed <- rownames(installed.packages())
for (pkg in required_packages) {
  if (!(pkg %in% installed)) {
    install.packages(pkg, dependencies = TRUE)
  }
}

# Load all packages
lapply(required_packages, library, character.only = TRUE)


```

```{r}
# Cricket Match Prediction - GLMM Analysis
# Load required libraries
library(lme4)
library(dplyr)
library(ggplot2)
library(performance)  # for model diagnostics
library(sjPlot)       # for plotting random effects

# Load data
data1 <- read.csv("cricket_model_data.csv")



# Convert to factors where appropriate
data1$season <- as.factor(data1$season)
data1$team1 <- as.factor(data1$team1)
data1$team2 <- as.factor(data1$team2)
data1$winner_binary <- as.factor(data1$winner_binary)

# Check class balance
table(data1$winner_binary)
prop.table(table(data1$winner_binary))


# ============================================================================
# MODEL 1: Baseline (simplest model)
# ============================================================================

model1 <- glmer(winner_binary ~ wickets_difference + runs_difference + 
                toss_won_by_team1 + (1 | season) + (1 | team1) + (1 | team2),
                data = data1,
                family = binomial(link = "logit"),
                control = glmerControl(optimizer = "bobyqa", optCtrl = list(maxfun = 100000)))

summary(model1)

# Check convergence
if (model1@optinfo$conv$opt == 0) {{
  print("Model 1 converged successfully")
}} else {{
  print("Warning: Model 1 convergence issue")
}}

# Model diagnostics
print(performance::check_collinearity(model1))


# ============================================================================
# MODEL 2: Extended model
# ============================================================================

model2 <- glmer(winner_binary ~ wickets_difference + runs_difference + 
                boundary_difference + team1_dot_ball_percentage + 
                toss_won_by_team1 + toss_decision_bat +
                (1 | season) + (1 | team1) + (1 | team2),
                data = data1,
                family = binomial(link = "logit"),
                control = glmerControl(optimizer = "bobyqa", optCtrl = list(maxfun = 100000)))

summary(model2)

if (model2@optinfo$conv$opt == 0) {{
  print("Model 2 converged successfully")
}} else {{
  print("Warning: Model 2 convergence issue")
}}


# ============================================================================
# MODEL 3: Hybrid model (separate wickets)
# ============================================================================

model3 <- glmer(winner_binary ~ runs_difference + team1_wickets + team2_wickets + 
                boundary_difference + team1_dot_ball_percentage +
                toss_won_by_team1 + toss_decision_bat +
                (1 | season) + (1 | team1) + (1 | team2),
                data = data1,
                family = binomial(link = "logit"),
                control = glmerControl(optimizer = "bobyqa", optCtrl = list(maxfun = 100000)))

summary(model3)


# ============================================================================
# MODEL COMPARISON
# ============================================================================

print("\\n=== MODEL COMPARISON ===\\n")

# AIC comparison
aic_values <- c(AIC(model1), AIC(model2), AIC(model3))
names(aic_values) <- c("Model 1", "Model 2", "Model 3")
print("AIC values:")
print(aic_values)
print(paste("Best model by AIC:", names(which.min(aic_values))))

# BIC comparison
bic_values <- c(BIC(model1), BIC(model2), BIC(model3))
names(bic_values) <- c("Model 1", "Model 2", "Model 3")
print("\\nBIC values:")
print(bic_values)
print(paste("Best model by BIC:", names(which.min(bic_values))))

# Likelihood ratio tests
print("\\nLikelihood Ratio Tests:")
print("Model 1 vs Model 2:")
print(anova(model1, model2))

print("\\nModel 2 vs Model 3:")
print(anova(model2, model3))


# ============================================================================
# RANDOM EFFECTS EXAMINATION (for best model - assuming model2)
# ============================================================================

print("\\n=== RANDOM EFFECTS VARIANCE ===\\n")

# Extract random effects
ranef_season <- ranef(model2)$season
ranef_team1 <- ranef(model2)$team1
ranef_team2 <- ranef(model2)$team2

print("Season random effects variance:")
print(var(ranef_season[,1]))

print("\\nTeam1 random effects variance:")
print(var(ranef_team1[,1]))

print("\\nTeam2 random effects variance:")
print(var(ranef_team2[,1]))

# Plot random effects
print("\\nPlotting random effects...")

# Season effects
season_df <- data.frame(
  season = rownames(ranef_season),
  effect = ranef_season[,1]
)

ggplot(season_df, aes(x = reorder(season, effect), y = effect)) +
  geom_point() +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  coord_flip() +
  labs(title = "Season Random Effects", x = "Season", y = "Random Effect") +
  theme_minimal()

ggsave("season_random_effects.png", width = 8, height = 10)

# Team effects (top 10 each)
team1_df <- data.frame(
  team = rownames(ranef_team1),
  effect = ranef_team1[,1]
) %>% arrange(desc(abs(effect))) %>% head(10)

ggplot(team1_df, aes(x = reorder(team, effect), y = effect)) +
  geom_point() +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  coord_flip() +
  labs(title = "Team1 Random Effects (Top 10)", x = "Team", y = "Random Effect") +
  theme_minimal()

ggsave("team1_random_effects.png", width = 8, height = 6)

```

```{r}
# ============================================================================
# MODEL PREDICTIONS & ACCURACY
# ============================================================================

print("\\n=== PREDICTION ACCURACY ===\\n")

# Get predictions from best model (model2)
data1$predicted_prob <- predict(model2, type = "response")
data1$predicted_class <- ifelse(data1$predicted_prob > 0.5, 1, 0)

# Confusion matrix
confusion <- table(Predicted = data1$predicted_class, Actual = data$winner_binary)
print("Confusion Matrix:")
print(confusion)

# Accuracy
accuracy <- sum(diag(confusion)) / sum(confusion)
print(paste("\\nAccuracy:", round(accuracy, 3)))

# Sensitivity and Specificity
sensitivity <- confusion[2,2] / sum(confusion[,2])
specificity <- confusion[1,1] / sum(confusion[,1])
print(paste("Sensitivity (Team1 wins):", round(sensitivity, 3)))
print(paste("Specificity (Team2 wins):", round(specificity, 3)))


# ============================================================================
# SAVE RESULTS
# ============================================================================

# Save model summaries
sink("glmm_results.txt")
print("\\n=== MODEL 1 SUMMARY ===")
print(summary(model1))
print("\\n=== MODEL 2 SUMMARY ===")
print(summary(model2))
print("\\n=== MODEL 3 SUMMARY ===")
print(summary(model3))
sink()

print("\\nResults saved to glmm_results.txt")
print("Plots saved as PNG files")
print("\\nAnalysis complete!")
```

```{r}
# ================================
# 1. Load libraries
# ================================
library(lme4)
library(dplyr)
library(caret)       # for train/test split
library(pROC)        # for ROC and AUC
library(MLmetrics)   # for F1, accuracy, etc.

# ================================
# 2. Train/Test Split
# ================================
set.seed(123)  # for reproducibility
train_index <- createDataPartition(data1$winner_binary, p = 0.8, list = FALSE)
train_data <- data1[train_index, ]
test_data  <- data1[-train_index, ]

# Check proportions
prop.table(table(train_data$winner_binary))
prop.table(table(test_data$winner_binary))

# ================================
# 3. Fit the Logistic Mixed Model on Training Data
# ================================
model_train <- glmer(
  winner_binary ~ wickets_difference + runs_difference + toss_won_by_team1 +
    (1 | season) + (1 | team1) + (1 | team2),
  data = train_data,
  family = binomial(link = "logit"),
  control = glmerControl(optimizer = "bobyqa", optCtrl = list(maxfun = 100000))
)

summary(model_train)

# ================================
# 4. Predict on Test Data
# ================================
# Predicted probabilities
test_data$pred_prob <- predict(model_train, newdata = test_data, type = "response", allow.new.levels = TRUE)

# Predicted classes (threshold = 0.5)
test_data$pred_class <- ifelse(test_data$pred_prob > 0.5, 1, 0)

# ================================
# 5. Performance Metrics
# ================================
# Accuracy
accuracy <- Accuracy(y_true = as.numeric(as.character(test_data$winner_binary)),
                     y_pred = test_data$pred_class)
print(paste("Accuracy:", round(accuracy, 4)))

# F1 Score
f1 <- F1_Score(y_true = as.numeric(as.character(test_data$winner_binary)),
               y_pred = test_data$pred_class)
print(paste("F1 Score:", round(f1, 4)))

# Confusion Matrix
conf_mat <- table(Predicted = test_data$pred_class, Actual = test_data$winner_binary)
print("Confusion Matrix:")
print(conf_mat)

# ROC Curve and AUC
roc_obj <- roc(as.numeric(as.character(test_data$winner_binary)), test_data$pred_prob)
auc_val <- auc(roc_obj)
print(paste("AUC:", round(auc_val, 4)))

# Plot ROC
plot(roc_obj, main = "ROC Curve for Logistic Mixed Model", col = "blue", lwd = 2)
abline(a = 0, b = 1, lty = 2, col = "red")

```


```{r}
library(lme4)
library(pROC)
library(dplyr)

set.seed(123)

seasons <- unique(data1$season)
n_seasons <- length(seasons)

# Store metrics
auc_values <- numeric(n_seasons)
accuracy_values <- numeric(n_seasons)

for (i in 1:n_seasons) {
  
  test_season <- seasons[i]
  
  train_data <- data1 %>% filter(season != test_season)
  test_data  <- data1 %>% filter(season == test_season)
  
  # Fit model
  model_cv <- glmer(
    winner_binary ~ wickets_difference + runs_difference + toss_won_by_team1 +
      (1 | season) + (1 | team1) + (1 | team2),
    data = train_data,
    family = binomial(link = "logit"),
    control = glmerControl(optimizer = "bobyqa", optCtrl = list(maxfun = 100000))
  )
  
  # Predict probabilities
  test_data$pred_prob <- predict(model_cv, newdata = test_data, type = "response", allow.new.levels = TRUE)
  test_data$pred_class <- ifelse(test_data$pred_prob > 0.5, 1, 0)
  
  # Metrics
  auc_values[i] <- auc(roc(as.numeric(as.character(test_data$winner_binary)), test_data$pred_prob))
  accuracy_values[i] <- mean(as.numeric(as.character(test_data$winner_binary)) == test_data$pred_class)
}

print(paste("Average AUC:", round(mean(auc_values), 4)))
print(paste("Average Accuracy:", round(mean(accuracy_values), 4)))

```
```{r}
# Test sequence for your model:
models_to_test <- list()

# 1. Base model (random intercepts only)
models_to_test$base <- glmer(
  winner_binary ~ wickets_difference + runs_difference + toss_won_by_team1 +
    (1 | season) + (1 | team1) + (1 | team2),
  data = train_data, family = binomial
)

# 2. Add toss effect varying by season
models_to_test$toss_season <- glmer(
  winner_binary ~ wickets_difference + runs_difference + toss_won_by_team1 +
    (1 + toss_won_by_team1 | team1) + (1 | team1) + (1 | team2),
  data = train_data, family = binomial
)

# 3. Compare
comparison <- anova(models_to_test$base, models_to_test$toss_season)
print(comparison)

# 4. Check if improvement is worth complexity
if (comparison$Pr[2] < 0.05) {
  cat("Random slope for toss by season is statistically justified\n")
} else {
  cat("Stick with random intercepts only\n")
}
```


